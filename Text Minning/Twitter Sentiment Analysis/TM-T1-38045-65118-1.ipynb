{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import re\n",
    "import textblob\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = './TextMinning/Tweets_EN_sentiment.json'\n",
    "DF = pd.read_json(FILE_PATH, orient = 'records', lines = True, encoding='utf8')\n",
    "\n",
    "DF['label'] = np.where(DF['class']=='pos', 1, -1)\n",
    "DF = DF.drop(['tweet', 'class'], axis = 1)\n",
    "DF['text_len'] = [len(x.split()) for x in DF['text']]\n",
    "DF = DF[DF['text_len'] > 2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Pre-Processing \n",
    "\n",
    "The code for ours PPTs is made with variations of the Cell underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE FROM:\n",
    "#https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#stemmer = nltk.stem.PorterStemmer()\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    #text = text.lower()\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    #stripped = re.sub(combined_pat, '', souped)\n",
    "    #try:\n",
    "    #    clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    #except:\n",
    "    #   clean = stripped\n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", souped)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tokenizer.tokenize(souped)\n",
    "    #stemmed = [stemmer.stem(word) for word in words]\n",
    "    lemmed = [lemmer.lemmatize(word) for word in words]\n",
    "    #snowball = [snowball_stemmer.stem(word) for word in words]\n",
    "    return lemmed\n",
    "   \n",
    "DF['text_cleaned'] = DF['text'].apply(lambda x: tweet_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    print(s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "DF['text_cleaned'] = DF['text_cleaned'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - TextBlob over whole DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['label_TB'] = DF['text'].apply(lambda x: textblob.TextBlob(x).sentiment.polarity)\n",
    "DF['label_TB'] = np.where(DF['label_TB']>=0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_mesure(LABELS, TESTS):\n",
    "    #print(LABELS.value_counts())\n",
    "    ACC = LABELS - TESTS\n",
    "    TOTAL = len(LABELS)\n",
    "    print('\\nACC from - (', LABELS.name, '/', TESTS.name, ')- total-', TOTAL)\n",
    "    REAL_POS = len(LABELS[LABELS == 1])\n",
    "    print(LABELS.name, '- positive -', REAL_POS, '- negative -', TOTAL - REAL_POS)\n",
    "    TESTS_POS = len(TESTS[TESTS == 1])\n",
    "    print(TESTS.name,'- positive -', TESTS_POS, '- negative -', TOTAL - TESTS_POS)\n",
    "    HITS = len(ACC[ACC == 0])  \n",
    "    print('Acurracy % of', round((HITS / TOTAL), 3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7968168752113629\n"
     ]
    }
   ],
   "source": [
    "#acc_mesure(DF['label'], DF['label_TB'])\n",
    "import sklearn.metrics as metrics\n",
    "print(\"Accuracy: \", metrics.accuracy_score(DF['label'], DF['label_TB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Lexicon Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICON_DF = pd.read_csv('./TextMinning/NCR-lexicon.csv', delimiter=';')\n",
    "LEXICON_DF['Value'] = ((LEXICON_DF['Negative'] * -1) + LEXICON_DF['Positive'])\n",
    "LEXICON = LEXICON_DF[['English', 'Value']].set_index('English')['Value'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14182\n"
     ]
    }
   ],
   "source": [
    "print(len(LEXICON))\n",
    "\n",
    "counts = []\n",
    "counts_lex = []\n",
    "\n",
    "def apply_lex(tokens):\n",
    "    lexSum = 0\n",
    "    #tokens = nltk.bigrams(tokens)\n",
    "    for word in tokens:\n",
    "        counts.append(word)\n",
    "        if word in LEXICON.keys():\n",
    "            lexSum += LEXICON.get(word)\n",
    "            counts_lex.append(word)\n",
    "    return lexSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7824230639161313\n"
     ]
    }
   ],
   "source": [
    "DF['lex_count'] = DF['text_cleaned'].apply(lambda row: apply_lex(row))\n",
    "DF['label_lex'] = np.where(DF['lex_count'] >= 0, 1, -1)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "print(\"Accuracy: \", metrics.accuracy_score(DF['label'], DF['label_lex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counts), len(counts_lex))\n",
    "freqCounts = nltk.FreqDist(counts)\n",
    "freqCountLex = nltk.FreqDist(counts_lex)\n",
    "print(len(freqCounts), len(freqCountLex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Vectorize - With CountVector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def count_words(tweets):\n",
    "    WORDS_COUNT = {}\n",
    "    for tweet in tweets:\n",
    "        #HERE DEFINE IF BIGRAMS OR NOT!!!!\n",
    "        #tweet = nltk.bigrams(tweet)\n",
    "        for word in tweet:\n",
    "            if word in WORDS_COUNT.keys():\n",
    "                WORDS_COUNT[word] += 1\n",
    "            else:\n",
    "                WORDS_COUNT[word] = 1\n",
    "    return collections.Counter(WORDS_COUNT)\n",
    "\n",
    "def vectorize(tweet, freqs):\n",
    "    valores = np.zeros([len(freqs)])\n",
    "    for word in tweet:\n",
    "        if word in freqs:\n",
    "            #valores[freqs.get(word)] = 1 #SE VECTOR DE 1 OU 0\n",
    "            valores[freqs.get(word)] = valores[freqs.get(word)] + 1 \n",
    "    return valores\n",
    "\n",
    "COUNT_VECT_DICT = count_words(DF['text_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience with 500 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8451865159040474\n",
      "Linear SVD - Accuracy:  0.8444467927718482\n",
      "Multinomial Naive Bayes - Accuracy:  0.839374405579626\n",
      "\n",
      "\n",
      "Experience with 1000 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8501532283630984\n",
      "Linear SVD - Accuracy:  0.8494135052308993\n",
      "Multinomial Naive Bayes - Accuracy:  0.840114128711825\n",
      "\n",
      "\n",
      "Experience with 2500 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8513156504279826\n",
      "Linear SVD - Accuracy:  0.8505759272957836\n",
      "Multinomial Naive Bayes - Accuracy:  0.8455035401035612\n",
      "\n",
      "\n",
      "Experience with 5000 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8530064461587235\n",
      "Linear SVD - Accuracy:  0.8420162739089084\n",
      "Multinomial Naive Bayes - Accuracy:  0.8496248546972419\n",
      "\n",
      "\n",
      "Experience with 7500 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8536404945577513\n",
      "Linear SVD - Accuracy:  0.8382119835147416\n",
      "Multinomial Naive Bayes - Accuracy:  0.8465602874352742\n",
      "\n",
      "\n",
      "Experience with 10000 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8527950966923808\n",
      "Linear SVD - Accuracy:  0.8355701151854592\n",
      "Multinomial Naive Bayes - Accuracy:  0.846454612702103\n",
      "\n",
      "\n",
      "Experience with 12500 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8534291450914087\n",
      "Linear SVD - Accuracy:  0.8373665856493713\n",
      "Multinomial Naive Bayes - Accuracy:  0.845080841170876\n",
      "\n",
      "\n",
      "Experience with 15000 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8532177956250661\n",
      "Linear SVD - Accuracy:  0.8363098383176583\n",
      "Multinomial Naive Bayes - Accuracy:  0.8444467927718482\n",
      "\n",
      "\n",
      "Experience with 17500 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8542745429567791\n",
      "Linear SVD - Accuracy:  0.8370495614498573\n",
      "Multinomial Naive Bayes - Accuracy:  0.8428616717742787\n",
      "\n",
      "\n",
      "Experience with 20000 most used words.\n",
      "Vectorizing Tweets!!!\n",
      "Train/Test Splinting\n",
      "LogisticRegression - Accuracy: 0.8551199408221494\n",
      "Linear SVD - Accuracy:  0.8377892845820565\n",
      "Multinomial Naive Bayes - Accuracy:  0.8434957201733065\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics as metrics\n",
    "ME_RES = []\n",
    "SVD_RES = []\n",
    "NB_RES = []\n",
    "\n",
    "for value in [500, 1000, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]:\n",
    "        \n",
    "        print('Experience with', value, 'most used words.')\n",
    "        EXP_DICT = COUNT_VECT_DICT.most_common(value)\n",
    "        INDEX_DICT = {}\n",
    "        \n",
    "        print('Vectorizing Tweets!!!')\n",
    "        for i, word in enumerate(EXP_DICT):\n",
    "            INDEX_DICT[EXP_DICT[i][0]] = i \n",
    "        DF['vector'] = DF['text_cleaned'].apply(lambda row: vectorize(row, INDEX_DICT))\n",
    "\n",
    "        print('Train/Test Splinting')\n",
    "        X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = sk.model_selection.train_test_split(\n",
    "                list(DF.vector),\n",
    "                list(DF.label),\n",
    "                train_size=0.80,\n",
    "                test_size=0.20,\n",
    "                random_state=1234)\n",
    "\n",
    "        lr = LogisticRegression()\n",
    "        model = lr.fit(X_TRAIN, Y_TRAIN)\n",
    "        y_pred = model.predict(X_TEST)\n",
    "        print('LogisticRegression - Accuracy:',  metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        ME_RES.append(metrics.accuracy_score(Y_TEST, y_pred))\n",
    "\n",
    "        svmc = LinearSVC(max_iter=1000)\n",
    "        model = svmc.fit(X_TRAIN, Y_TRAIN)\n",
    "        y_pred = model.predict(X_TEST)\n",
    "        print('Linear SVD - Accuracy: ', metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        SVD_RES.append(metrics.accuracy_score(Y_TEST, y_pred))\n",
    "\n",
    "        nb = MultinomialNB()\n",
    "        model = nb.fit(X_TRAIN, Y_TRAIN)\n",
    "        y_pred = model.predict(X_TEST)\n",
    "        print('Multinomial Naive Bayes - Accuracy: ', metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        NB_RES.append(metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkLearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tokenize = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "#tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "#sklearn_tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "sklearn_tfidf = TfidfVectorizer()\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(list(DF['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "sklearn_count = CountVectorizer(analyzer='word', binary=False, encoding='utf-8', \n",
    "                                input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "        tokenizer=None, vocabulary=None)\n",
    "sklearn_representation = sklearn_count.fit_transform(list(DF['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = sk.model_selection.train_test_split(\n",
    "        sklearn_representation,\n",
    "        list(DF.label),\n",
    "        train_size=0.80,\n",
    "        test_size=0.20,\n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8572334354855754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model = lr.fit(X_TRAIN, Y_TRAIN)\n",
    "y_pred = model.predict(X_TEST)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_TEST, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.858501532283631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "svmc = LinearSVC(max_iter=1000)\n",
    "model = svmc.fit(X_TRAIN, Y_TRAIN)\n",
    "y_pred = model.predict(X_TEST)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_TEST, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8329282468561767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "nb = MultinomialNB()\n",
    "model = nb.fit(X_TRAIN, Y_TRAIN)\n",
    "y_pred = model.predict(X_TEST)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_TEST, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "#docs = []\n",
    "\n",
    "def tf_idf(tweet):\n",
    "    doc = collections.Counter() \n",
    "    for word in tweet:\n",
    "         doc[word] += 1  \n",
    "    return(doc)\n",
    "\n",
    "TDIDF_COUNT = DF['text_cleaned'].apply(lambda row: tf_idf(row))\n",
    "\n",
    "def tf_idf2(docs):\n",
    "    tf = collections.Counter()\n",
    "    df = collections.Counter()\n",
    "    for d in docs:\n",
    "        #print(type(d))\n",
    "        for w in d:\n",
    "            tf[w] += d[w]\n",
    "            df[w] += 1\n",
    "    idfs = {}\n",
    "    for w in tf:\n",
    "        if tf[w] > 2:\n",
    "            idfs[w] = np.log(len(DF)/df[w])\n",
    "    #print(idfs)\n",
    "    return sorted(idfs, key=idfs.get, reverse=True)\n",
    "\n",
    "DICT = tf_idf2(list(DF['tfidf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet, freqs):\n",
    "    print(freqs)\n",
    "    valores = np.zeros([len(freqs)])\n",
    "    for word in tweet:\n",
    "        if word in freqs:\n",
    "            valores[freqs.get(word)] = 1 #SE VECTOR DE 1 OU 0\n",
    "            #valores[freqs.get(word)] = valores[freqs.get(word)] + 1 \n",
    "    return valores\n",
    "\n",
    "INDEX_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "ME_RES = []\n",
    "SVD_RES = []\n",
    "NB_RES = []\n",
    "\n",
    "for value in [500, 1000, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]:\n",
    "    \n",
    "        print('Experience with', value, 'most used words.')\n",
    "        #EXP_DICT = DICT.most_common(value)\n",
    "        EXP_DICT = tf_idf2(list(DF['tfidf']))\n",
    "        EXP_DICT = EXP_DICT[:value]\n",
    "        INDEX_DICT = {}\n",
    "        \n",
    "        print('Vectorizing Tweets!!!')\n",
    "        for i, word in enumerate(EXP_DICT):\n",
    "            INDEX_DICT[EXP_DICT[i][0]] = i \n",
    "        DF['vector'] = DF['text_cleaned'].apply(lambda row: vectorize(row, INDEX_DICT))\n",
    "\n",
    "        print('Train/Test Splinting')\n",
    "        X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = sk.model_selection.train_test_split(\n",
    "                list(DF.vector),\n",
    "                list(DF.label),\n",
    "                train_size=0.80,\n",
    "                test_size=0.20,\n",
    "                random_state=1234)\n",
    "\n",
    "        lr = LogisticRegression()\n",
    "        model = lr.fit(X_TRAIN, Y_TRAIN)\n",
    "        y_pred = model.predict(X_TEST)\n",
    "        print('LogisticRegression - Accuracy:',  metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        ME_RES.append(metrics.accuracy_score(Y_TEST, y_pred))\n",
    "\n",
    "        svmc = LinearSVC(max_iter=1000)\n",
    "        model = svmc.fit(X_TRAIN, Y_TRAIN)\n",
    "        y_pred = model.predict(X_TEST)\n",
    "        print('Linear SVD - Accuracy: ', metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        SVD_RES.append(metrics.accuracy_score(Y_TEST, y_pred))\n",
    "\n",
    "        nb = MultinomialNB()\n",
    "        model = nb.fit(X_TRAIN, Y_TRAIN)\n",
    "        y_pred = model.predict(X_TEST)\n",
    "        print('Multinomial Naive Bayes - Accuracy: ', metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        NB_RES.append(metrics.accuracy_score(Y_TEST, y_pred))\n",
    "        \n",
    "        print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
