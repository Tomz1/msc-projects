{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LsiModel, CoherenceModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, PCA, TruncatedSVD\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read.md\n",
    "The project had an iterative model development, thus we still have dependencies from cells to cells.\n",
    "We recomend running the project sequencialy from the begging to the end, and if wanted adding the printing and the visualizatios functions(word cloud and pyLDAvis when needed). \n",
    "\n",
    "For future work the use of spacy pipelines, will be used.\n",
    "\n",
    "PS - changes in models hyperparameters is possible (n_words, n_topic and tfidf params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_aux = []\n",
    "\n",
    "for i in reuters.fileids():\n",
    "    raw_doc =  reuters.raw(i)\n",
    "    raw_cat =  reuters.categories(i)\n",
    "    list_aux.append([i, raw_doc, raw_cat[0], len(raw_cat)])\n",
    "      \n",
    "df = pd.DataFrame(list_aux, columns=['id', 'raw_text', 'category', 'n_categories'])\n",
    "\n",
    "#doc_inference = df['raw_text'].sample(10).astype(str)\n",
    "doc_complete = df['raw_text'].astype(str)\n",
    "\n",
    "#doc_complete = df['raw_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "df.n_categories.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(df.category.value_counts()[:10].keys())\n",
    "#in case we want to use only X categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, n_words, n_topics):\n",
    "    results_df = pd.DataFrame([[word for rank, (word, prob) in enumerate(words)]\n",
    "                               for topic_id, words in model.show_topics(formatted=False,\n",
    "                                                        num_words=n_words, num_topics=n_topics)])\n",
    "    print(results_df)\n",
    "    print(results_df.to_latex())\n",
    "    return(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=doc_complete):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,2), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP.1\n",
    "\n",
    "    PP - lower, strip(4), numerics, stopwords(git)\n",
    "    tf - gensim\n",
    "    lsa / lda gensim ( 10 topics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Pre-Process\n",
    "stoplist = \"asian a a's able about above according accordingly across actually after afterwards again against ain't all allow allows almost alone along already also although always am among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren't around as aside ask asking associated at available away awfully b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c'mon c's came can can't cannot cant cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn't course currently d definitely described despite did didn't different do does doesn't doing don't done down downwards during e each edu eg eight either else elsewhere enough entirely especially et etc even ever every everybody everyone everything everywhere ex exactly example except f far few fifth first five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn't happens hardly has hasn't have haven't having he he's hello help hence her here here's hereafter hereby herein hereupon hers herself hi him himself his hither hopefully how howbeit however i i'd i'll i'm i've ie if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn't it it'd it'll it's its itself j just k keep keeps kept know knows known l last lately later latter latterly least less lest let let's like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly much must my myself n name namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right s said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn't since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would would wouldn't x y yes yet you you'd you'll you're you've your yours yourself yourselves z zero\"\n",
    "stoplist = stoplist.split()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def pre_process(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = gensim.parsing.preprocessing.strip_short(text, minsize=4)\n",
    "    text = gensim.parsing.preprocessing.strip_non_alphanum(text)\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stoplist]\n",
    "    \n",
    "    #text = remove_stop_words(text)\n",
    "    #stemmer = gensim.parsing.porter.PorterStemmer()\n",
    "    #text = stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "doc_clean = [pre_process(doc) for doc in doc_complete]\n",
    "df['clean_text'] = doc_clean\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP.1')\n",
    "id2word = corpora.Dictionary(doc_clean)\n",
    "corpus = [id2word.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5  # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, \n",
    "                                     dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()  # Compute Coherence Score\n",
    "print(round(time.time() - start), 'LDA Coherence: ', coherence_lda, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP.1.1 to show the pre-pross in SK and the Models in Gensim\n",
    "To convert a Sklearn Vectorizer to a Gensim corpus we used gensim.matutils.Sparse2Corpus (https://radimrehurek.com/gensim/matutils.html)\n",
    "\n",
    "    1) PP - exp1 \n",
    "    2) tf-idf - sklearn\n",
    "    3) lsa / lda gensim ( 10 topics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "print(round(time.time() - start), 'Starting Exp.3')\n",
    "doc_clean = [pre_process(doc) for doc in doc_complete]\n",
    "doc_clean = [\" \".join(doc) for doc in doc_clean]\n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP.1 as a string!')\n",
    "\n",
    "\n",
    "no_features = 10000  # SKLEARN TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=100, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(pd.Series(doc_clean))\n",
    "corpus = gensim.matutils.Sparse2Corpus(tfidf, documents_columns=False)\n",
    "#id2word = tfidf_vectorizer.vocabulary_\n",
    "id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5 # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity\n",
    "print(lda_results_df)\n",
    "print(lsa_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Visualize in pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(data,'v3.html')\n",
    "\n",
    "#print Word Clouds\n",
    "for t in range(lda_model.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(dict(lda_model.show_topic(t, 100))))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP.2\n",
    "Lematization based on: https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "\n",
    "    1) PP - exp1 + lem com POS TAG(Nomes, adj, verbos, adv)\n",
    "    2) tf - gensim\n",
    "    3) lsa / lda gensim ( 10 topics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "#doc_clean = [pre_process(doc) for doc in doc_complete]\n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP.1')\n",
    "\n",
    "doc_lemma =  lemmatization(doc_clean)\n",
    "df['lem_pos_text'] = doc_lemma\n",
    "\n",
    "print(round(time.time() - start), 'Docs. lemmatized and POS(w/ noun, adj, verb, adv)')\n",
    "\n",
    "id2word = corpora.Dictionary(doc_lemma)\n",
    "corpus = [id2word.doc2bow(doc) for doc in doc_lemma]\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5  # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "#print(round(time.time() - start), 'Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity\n",
    "#coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_lemma, dictionary=id2word, coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()  # Compute Coherence Score\n",
    "#print(round(time.time() - start), 'Coherence Score: ', coherence_lda, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp. 2.1 - w/ TF-IDF(sklearn)\n",
    "    1) PP - exp2\n",
    "    2) tf-idf - sklearn\n",
    "    3) lsa / lda - sklearn ( 10 topics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "doc_lemma_sk = [\" \".join(doc) for doc in doc_lemma]\n",
    "\n",
    "# SKLEARN TFIDF\n",
    "no_features = 10000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=100, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(pd.Series(doc_lemma_sk))\n",
    "#tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(tfidf, documents_columns=False)\n",
    "#id2word = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5 # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "#print(round(time.time() - start), 'Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, \n",
    "                                                  corpus=corpus, texts=df['clean_text'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "#df_dominant_topic.head(10)\n",
    "\n",
    "df['Infered_Topic'] = df_dominant_topic['Dominant_Topic']\n",
    "df['Keywords_Topic'] = df_dominant_topic['Keywords']\n",
    "\n",
    "pd.set_option('display.max_colwidth', 5)\n",
    "df[['id', 'category', 'n_categories', 'Infered_Topic', 'Keywords_Topic', 'clean_text']].head(2).to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 3 - NER\n",
    "\n",
    "    1) LEM + POS + NER + Stopwords \n",
    "    2) TF(gensin)\n",
    "    3) lsa / lda - sklearn (10 topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "stoplist = \"a a's able about above according accordingly across actually after afterwards again against ain't all allow allows almost alone along already also although always am among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren't around as aside ask asking associated at available away awfully b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c'mon c's came can can't cannot cant cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn't course currently d definitely described despite did didn't different do does doesn't doing don't done down downwards during e each edu eg eight either else elsewhere enough entirely especially et etc even ever every everybody everyone everything everywhere ex exactly example except f far few fifth first five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn't happens hardly has hasn't have haven't having he he's hello help hence her here here's hereafter hereby herein hereupon hers herself hi him himself his hither hopefully how howbeit however i i'd i'll i'm i've ie if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn't it it'd it'll it's its itself j just k keep keeps kept know knows known l last lately later latter latterly least less lest let let's like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly much must my myself n name namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible -PRON- presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right s said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn't since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would would wouldn't x y yes yet you you'd you'll you're you've your yours yourself yourselves z zero\"\n",
    "stoplist = stoplist.split()\n",
    "\n",
    "def ner(doc, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text = doc\n",
    "    text = gensim.parsing.preprocessing.strip_tags(text)\n",
    "    text = gensim.parsing.preprocessing.strip_multiple_whitespaces(text)\n",
    "    text = [word for word in text.split()]\n",
    "    \n",
    "    doc = nlp(\" \".join(text))\n",
    "    text = [token.lemma_ for token in doc \n",
    "                if token.pos_ in allowed_postags\n",
    "                or token.ent_type_]\n",
    "    \n",
    "    text = [word for word in text if word not in stoplist]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "doc_ner = [ner(doc) for doc in doc_complete]\n",
    "df['ner_text'] = doc_ner\n",
    "           \n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP. NER CLEANING')\n",
    "#print(round(time.time() - start), 'Docs. lemmatized and POS(w/ noun, adj, verb, adv)')\n",
    "\n",
    "id2word = corpora.Dictionary(doc_ner)\n",
    "corpus = [id2word.doc2bow(doc) for doc in doc_ner]\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5  # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_ner, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()  # Compute Coherence Score\n",
    "print(round(time.time() - start), 'Coherence Score: ', coherence_lda, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 3.1 - NER + SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ner_sk = [\" \".join(doc) for doc in doc_ner]\n",
    "\n",
    "# SKLEARN TFIDF\n",
    "no_features = 10000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=100, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(pd.Series(doc_ner_sk))\n",
    "#tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(tfidf, documents_columns=False)\n",
    "#id2word = tfidf_vectorizer.vocabulary_\n",
    "id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5 # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "#print(round(time.time() - start), 'Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, \n",
    "                                                  corpus=corpus, texts=df['ner_text'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "#df_dominant_topic.head(10)\n",
    "\n",
    "df['Infered_Topic'] = df_dominant_topic['Dominant_Topic']\n",
    "df['Keywords_Topic'] = df_dominant_topic['Keywords']\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df[['id', 'category', 'n_categories', 'Infered_Topic', 'Keywords_Topic', 'clean_text']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 4 - BIGRAMS + POS + NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "stoplist = \"000 & - a a's able about above according accordingly across actually after afterwards again against ain't all allow allows almost alone along already also although always am among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren't around as aside ask asking associated at available away awfully b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c'mon c's came can can't cannot cant cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn't course currently d definitely described despite did didn't different do does doesn't doing don't done down downwards during e each edu eg eight either else elsewhere enough entirely especially et etc even ever every everybody everyone everything everywhere ex exactly example except f far few fifth first five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn't happens hardly has hasn't have haven't having he he's hello help hence her here here's hereafter hereby herein hereupon hers herself hi him himself his hither hopefully how howbeit however i i'd i'll i'm i've ie if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn't it it'd it'll it's its itself j just k keep keeps kept know knows known l last lately later latter latterly least less lest let let's like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly much must my myself n name namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible -PRON- presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right s said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn't since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would would wouldn't x y yes yet you you'd you'll you're you've your yours yourself yourselves z zero\"\n",
    "stoplist = stoplist.split()\n",
    "\n",
    "bigram = gensim.models.Phrases(doc_complete, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def bi_ner(doc, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text = doc\n",
    "    text = gensim.parsing.preprocessing.strip_tags(text)\n",
    "    text = gensim.parsing.preprocessing.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    text = [word for word in text.split()]    \n",
    "    text = [word for word in text if word not in stoplist]\n",
    "    \n",
    "    text = bigram_mod[text]\n",
    "    \n",
    "    doc = nlp(\" \".join(text))\n",
    "    text = [token.lemma_ for token in doc \n",
    "                if token.pos_ in allowed_postags\n",
    "                or token.ent_type_]\n",
    "    return text\n",
    "\n",
    "\n",
    "doc_bi_ner = [bi_ner(doc) for doc in doc_complete]\n",
    "df['bi_ner_text'] = doc_bi_ner\n",
    "\n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP. NER CLEANING')\n",
    "#print(round(time.time() - start), 'Docs. lemmatized and POS(w/ noun, adj, verb, adv)')\n",
    "\n",
    "id2word = corpora.Dictionary(doc_bi_ner)\n",
    "corpus = [id2word.doc2bow(doc) for doc in doc_bi_ner]\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5  # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity\n",
    "#coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_ner, dictionary=id2word, coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()  # Compute Coherence Score\n",
    "#print(round(time.time() - start), 'Coherence Score: ', coherence_lda, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(data,'vlast.html')\n",
    "\n",
    "#print Word Clouds\n",
    "for t in range(lda_model.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(dict(lda_model.show_topic(t, 100))))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 4.1 - BIGRAMS + POS + NER + TFIDF(sk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "doc_ner_sk = [\" \".join(doc) for doc in doc_ner]\n",
    "# SKLEARN TFIDF\n",
    "no_features = 10000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=100, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(pd.Series(doc_ner_sk))\n",
    "#tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(tfidf, documents_columns=False)\n",
    "#id2word = tfidf_vectorizer.vocabulary_\n",
    "id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5 # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp. 5 - SKLEARN LDA\n",
    "    1) PP - exp1\n",
    "    2) tf-idf - sklearn\n",
    "    3) lsa / lda - sklearn ( 10 topics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "print(round(time.time() - start), 'Starting Exp.3')\n",
    "doc_clean = [pre_process(doc) for doc in doc_complete]\n",
    "doc_clean = [\" \".join(doc) for doc in doc_clean]\n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP.1 as a string!')\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5  # Represents the nº of words to show\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(doc_clean)\n",
    "lda_model = LatentDirichletAllocation(n_topics=n_topics,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1)      # compute perplexity\n",
    "\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=n_words) \n",
    "\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp. BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "def pre_process(text):\n",
    "    text = text.lower()\n",
    "    text = gensim.parsing.preprocessing.strip_non_alphanum(text)\n",
    "    text = text.split()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "doc_clean = [pre_process(doc) for doc in doc_complete]\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'Docs. cleaned w/ EXP.1')\n",
    "id2word = corpora.Dictionary(doc_clean)\n",
    "corpus = [id2word.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "n_topics  = 10  # Variable to Change the n. of topics\n",
    "n_words = 5  # Represents the nº of words to show\n",
    "\n",
    "print(round(time.time() - start), 'Creating LDA and LSA Models w/ Preprossesing EXP.1.')\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "lsa_model = LsiModel(corpus=corpus, num_topics=n_topics, id2word=id2word)\n",
    "\n",
    "print(round(time.time() - start), 'Models Created w/', n_topics, 'Topics.')\n",
    "\n",
    "print(round(time.time() - start), 'LDA Results in DF format. \\n')\n",
    "print_model(lda_model, n_words, n_topics)\n",
    "print(round(time.time() - start), 'LSA Results in DF format. \\n')\n",
    "print_model(lsa_model, n_words, n_topics)\n",
    "\n",
    "\n",
    "print(round(time.time() - start), 'LDA Perplexity: ', lda_model.log_perplexity(corpus))  # Compute Perplexity\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, \n",
    "                                     dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()  # Compute Coherence Score\n",
    "print(round(time.time() - start), 'LDA Coherence: ', coherence_lda, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
